<?xml version="1.0" encoding="UTF-8"?>
<doc xmlns:aid="http://ns.adobe.com/AdobeInDesign/4.0/"><title aid:pstyle="h1">第5章　トラジェクトリベースな声質変換</title><?dtp level="1" section="第5章　トラジェクトリベースな声質変換"?>
<p>前章では、最も素朴な声質変換手法について説明したが、これには1つ大きな問題点がある。それは、前後フレームとの関連性を全く考慮せずにフレームごとに独立に変換しているために、グラフでも推移がギザギザになっていた通り、自然性(声のナチュラルさ)が失われてしまっているという点である。</p>
<p>この章では、前後フレームとのつながりを考慮した、トラジェクトリベースな変換と呼ばれる変換手法について解説する。なお、この章についても、戸田氏の論文<span type='bibref' idref='toda-traj'>[6]</span>及び、r9y9氏のブログ<span type='bibref' idref='r9y9-gmm'>[7]</span>を参照した。</p>
<title aid:pstyle="h2">5.1　動的特徴量の導入</title><?dtp level="2" section="5.1　動的特徴量の導入"?>
<p>前後フレームとのつながりを考慮した変換を行うために、MFCCの抽出の際に実装した動的特徴量を導入する。ここでは、動的特徴量<replace idref="texinline-1"><pre>\Delta \boldsymbol{x}_t</pre></replace>を以下のように定義する。</p>
<replace idref="texblock-1">
<pre>
    \Delta \boldsymbol{x}_t = \frac{1}{2} (\boldsymbol{x}_{t + 1} - \boldsymbol{x}_{t - 1})
</pre>
</replace>
<p>そして、前章での変換元、変換先の特徴量の代わりに、動的変化量を結合した特徴量<replace idref="texinline-2"><pre>\boldsymbol{X}_t =  [\boldsymbol{x}_t ^ \top, \Delta \boldsymbol{x}_t ^ \top ], \boldsymbol{Y}_t =  [\boldsymbol{y}_t ^ \top, \Delta \boldsymbol{y}_t ^ \top ]</pre></replace>を用いる。時系列で結合した全体の特徴量は、以下の通りとなる。</p>
<replace idref="texblock-2">
<pre>
    \boldsymbol{X} = [\boldsymbol{X}_1 ^ \top, \boldsymbol{X}_2 ^ \top, ..., \boldsymbol{X}_t ^ \top, ..., \boldsymbol{X}_T ^ \top], \
    \boldsymbol{Y} = [\boldsymbol{Y}_1 ^ \top, \boldsymbol{Y}_2 ^ \top, ..., \boldsymbol{Y}_t ^ \top, ..., \boldsymbol{Y}_T ^ \top]
</pre>
</replace>
<p>この時、<replace idref="texinline-3"><pre>\boldsymbol{x}_t, \boldsymbol{y}_t</pre></replace>が共に<replace idref="texinline-4"><pre>D</pre></replace>次元のベクトルだとすると、<replace idref="texinline-5"><pre>\boldsymbol{X}, \boldsymbol{Y}</pre></replace>は共に<replace idref="texinline-6"><pre>2D \times T</pre></replace>次元の行列となる。</p>
<p>また、前章と同様に変換元と変換先の特徴量を結合した<replace idref="texinline-7"><pre>4D</pre></replace>次元の特徴量<replace idref="texinline-8"><pre>\boldsymbol{Z}_t = [\boldsymbol{X}_t ^ \top, \boldsymbol{Y}_t ^ \top]</pre></replace>を用いてGMMの学習処理を行い、<replace idref="texinline-9"><pre>\boldsymbol{\lambda} ^ {(Z)}</pre></replace>を求める。</p>
<title aid:pstyle="h3">学習処理の実装</title><?dtp level="3" section="学習処理の実装"?>
<p>学習処理については、特徴量に動的変化量を結合する以外には、前章との大きな違いはない。ここでも、変換処理はTrajectoryGMMMapというクラスで行うものとして、まず学習処理のみを実装する。</p>
<codelist>
<caption>リスト5.1　learn_trajectory.py</caption>
<pre><span type='lineno'> 1: </span>#!/usr/bin/env python
<span type='lineno'> 2: </span>
<span type='lineno'> 3: </span>import math
<span type='lineno'> 4: </span>import numpy
<span type='lineno'> 5: </span>import os
<span type='lineno'> 6: </span>import pickle
<span type='lineno'> 7: </span>import re
<span type='lineno'> 8: </span>import sklearn
<span type='lineno'> 9: </span>import sklearn.mixture
<span type='lineno'>10: </span>import sys
<span type='lineno'>11: </span>
<span type='lineno'>12: </span>from trajectory import TrajectoryGMMMap
<span type='lineno'>13: </span>
<span type='lineno'>14: </span>from stf import STF
<span type='lineno'>15: </span>from mfcc import MFCC
<span type='lineno'>16: </span>from dtw import DTW
<span type='lineno'>17: </span>
<span type='lineno'>18: </span>D = 16
<span type='lineno'>19: </span>M = 32
<span type='lineno'>20: </span>
<span type='lineno'>21: </span>if __name__ == '__main__':
<span type='lineno'>22: </span>    if len(sys.argv) &lt; 5:
<span type='lineno'>23: </span>        print 'Usage: %s [list of source stf] [list of target stf] ' + \
<span type='lineno'>24: </span>                            [dtw cache directory] [output file]' % sys.argv[0]
<span type='lineno'>25: </span>        sys.exit()
<span type='lineno'>26: </span>
<span type='lineno'>27: </span>    source_list = open(sys.argv[1]).read().strip().split('\n')
<span type='lineno'>28: </span>    target_list = open(sys.argv[2]).read().strip().split('\n')
<span type='lineno'>29: </span>
<span type='lineno'>30: </span>    assert len(source_list) == len(target_list)
<span type='lineno'>31: </span>
<span type='lineno'>32: </span>    learn_data = None
<span type='lineno'>33: </span>
<span type='lineno'>34: </span>    for i in xrange(len(source_list)):
<span type='lineno'>35: </span>        source = STF()
<span type='lineno'>36: </span>        source.loadfile(source_list[i])
<span type='lineno'>37: </span>
<span type='lineno'>38: </span>        mfcc = MFCC(source.SPEC.shape[1] * 2, source.frequency, dimension = D)
<span type='lineno'>39: </span>        source_mfcc = numpy.array([mfcc.mfcc(source.SPEC[frame]) \
<span type='lineno'>40: </span>                                    for frame in xrange(source.SPEC.shape[0])])
<span type='lineno'>41: </span>
<span type='lineno'>42: </span>        target = STF()
<span type='lineno'>43: </span>        target.loadfile(target_list[i])
<span type='lineno'>44: </span>
<span type='lineno'>45: </span>        mfcc = MFCC(target.SPEC.shape[1] * 2, target.frequency, dimension = D)
<span type='lineno'>46: </span>        target_mfcc = numpy.array([mfcc.mfcc(target.SPEC[frame]) \
<span type='lineno'>47: </span>                                    for frame in xrange(target.SPEC.shape[0])])
<span type='lineno'>48: </span>
<span type='lineno'>49: </span>        cache_path = os.path.join(sys.argv[3], '%s_%s.dtw' \
<span type='lineno'>50: </span>            % tuple(map(lambda x: re.sub('[./]', '_', re.sub('^[./]*', '', x)), \
<span type='lineno'>51: </span>                                                [source_list[i], target_list[i]])))
<span type='lineno'>52: </span>        if os.path.exists(cache_path):
<span type='lineno'>53: </span>            dtw = pickle.load(open(cache_path))
<span type='lineno'>54: </span>        else:
<span type='lineno'>55: </span>            dtw = DTW(source_mfcc, target_mfcc, \
<span type='lineno'>56: </span>                    window = abs(source.SPEC.shape[0] - target.SPEC.shape[0]) * 2)
<span type='lineno'>57: </span>            with open(cache_path, 'wb') as output:
<span type='lineno'>58: </span>                pickle.dump(dtw, output)
<span type='lineno'>59: </span>
<span type='lineno'>60: </span>        warp_mfcc = dtw.align(source_mfcc)
<span type='lineno'>61: </span>
<span type='lineno'>62: </span>        # 変換元、変換先共に、動的変化量を結合する
<span type='lineno'>63: </span>        warp_data = numpy.hstack([warp_mfcc, mfcc.delta(warp_mfcc)])
<span type='lineno'>64: </span>        target_data = numpy.hstack([target_mfcc, mfcc.delta(target_mfcc)])
<span type='lineno'>65: </span>
<span type='lineno'>66: </span>        data = numpy.hstack([warp_data, target_data])
<span type='lineno'>67: </span>        if learn_data is None:
<span type='lineno'>68: </span>            learn_data = data
<span type='lineno'>69: </span>        else:
<span type='lineno'>70: </span>            learn_data = numpy.vstack([learn_data, data])
<span type='lineno'>71: </span>
<span type='lineno'>72: </span>    gmm = sklearn.mixture.GMM(n_components = M, covariance_type = 'full')
<span type='lineno'>73: </span>    gmm.fit(learn_data)
<span type='lineno'>74: </span>
<span type='lineno'>75: </span>    gmmmap = TrajectoryGMMMap(gmm, learn_data.shape[0])
<span type='lineno'>76: </span>
<span type='lineno'>77: </span>    with open(sys.argv[4], 'wb') as output:
<span type='lineno'>78: </span>        pickle.dump(gmmmap, output)
</pre></codelist>
<title aid:pstyle="h2">5.2　トラジェクトリベースな変換処理</title><?dtp level="2" section="5.2　トラジェクトリベースな変換処理"?>
<title aid:pstyle="h3">時系列に結合した特徴量に対する確率密度関数</title><?dtp level="3" section="時系列に結合した特徴量に対する確率密度関数"?>
<p>前章では、フレームごとに確率密度関数を求め、変換処理を行っていたが今回は前後でのつながりを考えるために、<replace idref="texinline-10"><pre>\boldsymbol{X}, \boldsymbol{Y}</pre></replace>に関する確率密度関数を考える。<replace idref="texinline-11"><pre>\boldsymbol{X}, \boldsymbol{Y}</pre></replace>は単純にフレームごとの特徴量を時系列でつなげたものなので、前章で用いた確率密度関数のすべてのフレームにおける積を考えればよい。</p>
<replace idref="texblock-3">
<pre>
    P(\boldsymbol{Y} | \boldsymbol{X}, \boldsymbol{\lambda} ^ {(Z)}) = \prod_{t=1}^{T} \sum_{m=1}^{M} P(m| \boldsymbol{X}_t, \boldsymbol{\lambda} ^ {(Z)}) P(\boldsymbol{Y}_t | \boldsymbol{X}_t, m, \boldsymbol{\lambda} ^ {(Z)})
</pre>
</replace>
<p>このとき、前章と同様に、<replace idref="texinline-12"><pre>P(m| \boldsymbol{X}_t, \boldsymbol{\lambda} ^ {(Z)})</pre></replace>及び<replace idref="texinline-13"><pre>P(\boldsymbol{Y}_t | \boldsymbol{X}_t, m, \boldsymbol{\lambda} ^ {(Z)})</pre></replace>は以下のように表すことができる。</p>
<replace idref="texblock-4">
<pre>
    P(m | \boldsymbol{X}_t, \lambda ^ {(Z)}) = \frac{ w_m \mathcal{N}(\boldsymbol{X}_t; \boldsymbol{\mu}_m ^ {(Z)}, \boldsymbol{\Sigma}_m ^ {(Z)}) }{ \sum_{n = 1}^{M} w_n \mathcal{N}(\boldsymbol{X}_t, \boldsymbol{\mu}_n ^ {(Z)}, \boldsymbol{\Sigma}_n ^ {(Z)}) }
</pre>
</replace>
<replace idref="texblock-5">
<pre>
    P(\boldsymbol{Y}_t | \boldsymbol{X}_t, m, \boldsymbol{\lambda} ^ {(Z)}) = \mathcal{N} (\boldsymbol{Y}_t; \boldsymbol{E}_{m, t} ^ {(Y)}, \boldsymbol{D}_m ^ {(Y)})
</pre>
</replace>
<p>ここで、<replace idref="texinline-14"><pre>\boldsymbol{E}_{m, t} ^ {(y)}</pre></replace>及び<replace idref="texinline-15"><pre>\boldsymbol{D}_m^{(y)}</pre></replace>は以下の通りである。</p>
<replace idref="texblock-6">
<pre>
    \begin{gathered}
        \boldsymbol{E}_{m, t} ^ {(Y)} = \boldsymbol{\mu}_m ^ {(Y)} + \boldsymbol{\Sigma}_m ^ {(YX)} {\boldsymbol{\Sigma}_m ^ {(XX)}} ^ {-1} (\boldsymbol{X}_t - \boldsymbol{\mu}_m ^ {(X)}) \\
        \boldsymbol{D}_m ^ {(Y)} = \boldsymbol{\Sigma}_m ^ {(YY)} + \boldsymbol{\Sigma}_m ^ {(YX)} {\boldsymbol{\Sigma}_m ^ {(XX)}} ^ {-1} \boldsymbol{\Sigma}_m ^ {(XY)}
    \end{gathered}
</pre>
</replace>
<p>また、確率密度関数<replace idref="texinline-16"><pre>P(\boldsymbol{X} | \boldsymbol{Y}, \boldsymbol{\lambda} ^ {(Z)})</pre></replace>は、分布系列<replace idref="texinline-17"><pre>\boldsymbol{m} = \lbrace m_1, m_2, ..., m_t, ..., m_T \rbrace</pre></replace>を用いて、以下のように表すことができる。</p>
<replace idref="texblock-7">
<pre>
    P(\boldsymbol{Y} | \boldsymbol{X}, \boldsymbol{\lambda} ^ {(Z)}) = \sum_{all \  \boldsymbol{m}} P(\boldsymbol{m} | \boldsymbol{X}_t, \boldsymbol{\lambda} ^ {(Z)}) P(\boldsymbol{Y}_t | \boldsymbol{X}_t, \boldsymbol{m}, \boldsymbol{\lambda} ^ {(Z)})
</pre>
</replace>
<p>この確率密度関数に基づいて、求める特徴量<replace idref="texinline-18"><pre>\hat{\boldsymbol{y}}</pre></replace>は以下のように定式化できる。</p>
<replace idref="texblock-8">
<pre>
    \hat{\boldsymbol{y}} = \underset{\boldsymbol{y}}{argmax} \ P(\boldsymbol{Y} | \boldsymbol{X}, \boldsymbol{\lambda} ^ {(Z)})
</pre>
</replace>
<p>ただし、この確率密度関数は<replace idref="texinline-19"><pre>\boldsymbol{X}</pre></replace>が与えられた時の<replace idref="texinline-20"><pre>\boldsymbol{Y}</pre></replace>についての確率を記述したものなので、<replace idref="texinline-21"><pre>\boldsymbol{y}</pre></replace>と<replace idref="texinline-22"><pre>\boldsymbol{Y}</pre></replace>の関係性を与える必要がある。そこで、以下のように定義される変換行列<replace idref="texinline-23"><pre>\boldsymbol{W}</pre></replace>を導入する。</p>
<replace idref="texblock-9">
<pre>
    \begin{gathered}
        \boldsymbol{W} = [\boldsymbol{W}_1, \boldsymbol{W}_2, ..., \boldsymbol{W}_t, ..., \boldsymbol{W}_T] ^ \top \otimes \boldsymbol{I}_{\boldsymbol{D} \times \boldsymbol{D}} \\
        \boldsymbol{W}_t = [\boldsymbol{w}_t ^ {(0)}, \boldsymbol{w}_t ^ {(1)}] \\
        \boldsymbol{w}_t ^ {(0)} = [\overset{1st}{0}, \overset{2nd}{0}, ..., \overset{t - 1\ th}{0}, \overset{t\ th}{1}, \overset{t + 1\ th}{0}, ..., \overset{T\ th}{0}] \\
        \boldsymbol{w}_t ^ {(1)} = [\overset{1st}{0}, \overset{2nd}{0}, ..., \overset{t - 1\ th}{-0.5}, \overset{t\ th}{0}, \overset{t + 1\ th}{0.5}, ..., \overset{T\ th}{0}]
    \end{gathered}
</pre>
</replace>
<p>ここで、各フレームについて<replace idref="texinline-24"><pre>\boldsymbol{W}_t</pre></replace>が<replace idref="texinline-25"><pre>\boldsymbol{y}_t</pre></replace>から<replace idref="texinline-26"><pre>\boldsymbol{Y}_t</pre></replace>への変換を担っているが、<replace idref="texinline-27"><pre>\boldsymbol{w}_t ^ {(0)}</pre></replace>が<replace idref="texinline-28"><pre>\boldsymbol{y}_t</pre></replace>をそのまま移し、<replace idref="texinline-29"><pre>\boldsymbol{w}_t ^ {(1)}</pre></replace>が、この章の冒頭で定義した動的特徴量と対応していることが分かる。ただし、実装に合わせるために、<replace idref="texinline-30"><pre>\Delta \boldsymbol{x}_0 = \frac{1}{2} (\boldsymbol{x}_{1} - \boldsymbol{x}_{0})</pre></replace>及び<replace idref="texinline-31"><pre>\Delta \boldsymbol{x}_T = \frac{1}{2} (\boldsymbol{x}_{T} - \boldsymbol{x}_{T - 1})</pre></replace>とし、変換行列についても、<replace idref="texinline-32"><pre>\boldsymbol{W}_0, \boldsymbol{W}_T</pre></replace>を以下の図のように設定する。</p>
<img>
<Image href="file://images/w-matrix.png" scale="0.25" />
<caption>図5.1　変換行列Wのイメージ(<span type='bibref' idref='toda-traj'>[6]</span>にあった図を元に編集)</caption>
</img>
<p>元論文<span type='bibref' idref='toda-traj'>[6]</span>では、EMアルゴリズムを用いて、この定式化に基づいた変換パラメータの導出方法も紹介されているが、ここでは準最適な分布系列を用いて計算量を削減する手法を用いる。これは、それぞれのフレームごとに用いるガウス分布を1つにするということで、つまり、分布系列<replace idref="texinline-33"><pre>\boldsymbol{m}</pre></replace>を準最適な<replace idref="texinline-34"><pre>\hat{\boldsymbol{m}} = [\hat{m}_1, \hat{m}_2, ..., \hat{m}_t, ..., \hat{m}_T]</pre></replace>の1つに固定することによって、以下のように近似する。</p>
<replace idref="texblock-10">
<pre>
    P(\boldsymbol{Y} | \boldsymbol{X}, \boldsymbol{\lambda} ^ {(Z)}) \simeq P(\hat{\boldsymbol{m}} | \boldsymbol{X}_t, \boldsymbol{\lambda} ^ {(Z)}) P(\boldsymbol{Y}_t | \boldsymbol{X}_t, \hat{\boldsymbol{m}}, \boldsymbol{\lambda} ^ {(Z)})
</pre>
</replace>
<p>このとき、<replace idref="texinline-35"><pre>\hat{\boldsymbol{m}}</pre></replace>は以下のように決定される。</p>
<replace idref="texblock-11">
<pre>
    \hat{\boldsymbol{m}} = \underset{\boldsymbol{m}}{argmax} \ P(\boldsymbol{m} | \boldsymbol{X}, \boldsymbol{\lambda} ^ {(Z)})
</pre>
</replace>
<p>これらをまとめると、求める特徴量は以下のように表される。</p>
<replace idref="texblock-12">
<pre>
    \begin{split}
        \hat{\boldsymbol{y}} &amp; = \underset{\boldsymbol{y}}{argmax} \ P(\boldsymbol{Y} | \boldsymbol{X}, \boldsymbol{\lambda} ^ {(Z)}) \ subject \ to \ \boldsymbol{Y} = \boldsymbol{W} \boldsymbol{y} \\
                             &amp; \simeq P(\hat{\boldsymbol{m}} | \boldsymbol{X}_t, \boldsymbol{\lambda} ^ {(Z)}) P(\boldsymbol{Y}_t | \boldsymbol{X}_t, \hat{\boldsymbol{m}}, \boldsymbol{\lambda} ^ {(Z)}) \ subject \ to \ \boldsymbol{Y} = \boldsymbol{W} \boldsymbol{y} \\
                             &amp; \qquad \qquad \qquad \qquad \qquad where \ \hat{\boldsymbol{m}} = \underset{\boldsymbol{m}}{argmax} \ P(\boldsymbol{m} | \boldsymbol{X}, \boldsymbol{\lambda} ^ {(Z)})
    \end{split}
</pre>
</replace>
<title aid:pstyle="h3">変換特徴量の導出</title><?dtp level="3" section="変換特徴量の導出"?>
<p>求めた確率密度関数に基づいて、最尤推定を行い、変換後の特徴量を導出する。対数尤度を考えることによって、以下のように特徴量を求めることができるが、具体的な導出手法については筆者の理解の浅さと7時間後に迫った締め切りのために割愛させていただく。詳しくは、元論文<span type='bibref' idref='toda-traj'>[6]</span>のAppendixを参照してほしい。</p>
<replace idref="texblock-13">
<pre>
    \hat{\boldsymbol{y}} = (\boldsymbol{W} ^ \top {\boldsymbol{D}_{\hat{\boldsymbol{m}}} ^ {(Y)}} ^ {-1} \boldsymbol{W}) ^ {-1} \boldsymbol{W} ^ \top {\boldsymbol{D}_{\hat{\boldsymbol{m}}} ^ {(Y)}} ^ {-1} \boldsymbol{E}_{\hat{\boldsymbol{m}}} ^ {(Y)}
</pre>
</replace>
<p>ここで、<replace idref="texinline-36"><pre>\boldsymbol{E}_{\hat{\boldsymbol{m}}} ^ {(Y)}, {\boldsymbol{D}_{\hat{\boldsymbol{m}}} ^ {(Y)}} ^ {-1}</pre></replace>は以下のように与えられる。</p>
<replace idref="texblock-14">
<pre>
    \begin{gathered}
        \boldsymbol{E}_{\hat{\boldsymbol{m}}} ^ {(Y)} = [\boldsymbol{E}_{\hat{m}_1, 1} ^ {(Y)}, \boldsymbol{E}_{\hat{m}_2, 2} ^ {(Y)}, ..., \boldsymbol{E}_{\hat{m}_t, t} ^ {(Y)}, ..., \boldsymbol{E}_{\hat{m}_T, T} ^ {(Y)}] \\
        {\boldsymbol{D}_{\hat{\boldsymbol{m}}} ^ {(Y)}} ^ {-1} = diag \ [{\boldsymbol{D}_{\hat{\boldsymbol{m_1}}} ^ {(Y)}} ^ {-1}, {\boldsymbol{D}_{\hat{\boldsymbol{m_2}}} ^ {(Y)}} ^ {-1}, ..., {\boldsymbol{D}_{\hat{\boldsymbol{m_t}}} ^ {(Y)}} ^ {-1}, ..., {\boldsymbol{D}_{\hat{\boldsymbol{m_T}}} ^ {(Y)}} ^ {-1}]
    \end{gathered}
</pre>
</replace>
<title aid:pstyle="h3">変換処理の実装</title><?dtp level="3" section="変換処理の実装"?>
<p>導出結果に基づいて、変換処理の実装を行う。具体的には、学習処理の項でも述べたように、GMMMapクラスを継承したTrajectoryGMMMapクラスを実装する。</p>
<p>まず、コンストラクタにて、変換元のデータと独立で求められる<replace idref="texinline-37"><pre>\boldsymbol{D}_m ^ {(Y)}</pre></replace>の計算処理を行う。</p>
<list type='emlist'><caption aid:pstyle='emlist-title'>行列Dの算出</caption>
<pre>    def __init__(self, gmm, swap = False):
        # GMMMapのコンストラクタを呼び出す
        super(TrajectoryGMMMap, self).__init__(gmm, swap)

        D = gmm.means_.shape[1] / 2

        # すべてのmについてP(Y|X)における分散共分散行列Dをまとめて1つの変数として扱う
        self.D = np.zeros((self.M, D, D))
        for m in range(self.M):
            xx_inv_xy = np.linalg.solve(self.covarXX[m], self.covarXY[m])
            self.D[m] = self.covarYY[m] - np.dot(self.covarYX[m], xx_inv_xy)
</pre></list>
<p>次に、変換処理を実装するにあたり、<replace idref="texinline-38"><pre>\boldsymbol{y}</pre></replace>から<replace idref="texinline-39"><pre>\boldsymbol{Y}</pre></replace>への変換行列<replace idref="texinline-40"><pre>\boldsymbol{W}</pre></replace>を生成するメソッドを実装する。<replace idref="texinline-41"><pre>\boldsymbol{W}</pre></replace>の生成は、<replace idref="texinline-42"><pre>T</pre></replace>回のループで<replace idref="texinline-43"><pre>\boldsymbol{w}_t ^ {(0)}, \boldsymbol{w}_t ^ {(1)}</pre></replace>を生成し、つなげていくことで行っている。</p>
<p>また、ここで注意すべきなのはscipy.sparseモジュールを使うことで高速化を図っているという点である。scipy.sparseは疎行列を扱うためのモジュールで、<replace idref="texinline-44"><pre>\boldsymbol{W}</pre></replace>は<span type='image'>図5.1</span>を見れば分かるように対角成分付近以外はほとんどが0の疎行列なので、これを用いることで計算量や必要なメモリを減らすことができる。</p>
<p>scipy.sparseモジュールには幾つかの疎行列の実装があるが、ここではlil_matrixとcsr_matrixの2つを用いている。lil_matrixではインデックスを指定してデータを割り当てができる一方で、行列に対する計算操作はcsr_matrixの方が高速なので、lil_matrixで行列の生成をした後にcsr_matrixへの変換を行っている。</p>
<list type='emlist'><caption aid:pstyle='emlist-title'>行列Wの算出</caption>
<pre>    def __construct_weight_matrix(self, T, D):
        W = None

        for t in range(T):
            # 図の各行に対応する行列を生成する
            w0 = scipy.sparse.lil_matrix((D, D * T))
            w1 = scipy.sparse.lil_matrix((D, D * T))

            # scipy.sparse.diagsを使って図の「1」のマスに該当する部分の対角成分に1を代入する
            w0[0:, t * D: (t + 1) * D] = scipy.sparse.diags(np.ones(D), 0)

            # 図の「-0.5」のマスに該当する部分の対角成分に-0.5を代入する
            tmp = np.zeros(D).fill(-0.5)
            if t == 0:
                w1[0:, :D] = scipy.sparse.diags(tmp, 0)
            else:
                # t == 0でない場合はt - 1番目のマスに代入する
                w1[0:, (t - 1) * D: t * D] = scipy.sparse.diags(tmp, 0)

            # 図の「0.5」のマスに該当する部分の対角成分に0.5を代入する
            tmp = np.zeros(D).fill(-0.5)
            if t == T - 1:
                w1[0:, t * D:] = scipy.sparse.diags(tmp, 0)
            else:
                # t == 1でない場合はt + 1番目のマスに代入する
                w1[0:, (t + 1) * D: (t + 2) * D] = scipy.sparse.diags(tmp, 0)

            # w0とw1を結合したものを積み重ねていく
            W_t = scipy.sparse.vstack([w0, w1])
            if W == None:
                W = W_t
            else:
                W = scipy.sparse.vstack([W, W_t])

        # 最後にlil_matrixをcsr_matrixへと変換する
        return W.tocsr()
</pre></list>
<p>最後に、変換処理本体を実装する。まず、変換行列<replace idref="texinline-45"><pre>\boldsymbol{W}</pre></replace>を生成した後に、<replace idref="texinline-46"><pre>\hat{\boldsymbol{m}}</pre></replace>を求め、それに基づいて<replace idref="texinline-47"><pre>\boldsymbol{E}_{\hat{\boldsymbol{m}}} ^ {(Y)}, {\boldsymbol{D}_{\hat{\boldsymbol{m}}} ^ {(Y)}} ^ {-1}</pre></replace>を計算する。そして、求めた行列の積をscipy.sparse.linalg.spsolveによって計算し、特徴量を求める。</p>
<p>注意すべき点の1つは、準最適な分布系列<replace idref="texinline-48"><pre>\hat{\boldsymbol{m}}</pre></replace>を求める際に、sklearn.mixture.GMMのpredictメソッドを用いているということである。<replace idref="texinline-49"><pre>\hat{m}_t</pre></replace>は<replace idref="texinline-50"><pre>t</pre></replace>番目のフレームにおける、最も事後確率が高いガウス分布のインデックスと同義なので、predictメソッドが返すラベルをそのまま用いることができるのだ。</p>
<list type='emlist'><caption aid:pstyle='emlist-title'>変換処理</caption>
<pre>    def convert(self, src):
        T, D = src.shape[0], src.shape[1] / 2
        W = self.__construct_weight_matrix(T, D)

        # 準最適な分布系列を求める
        optimum_mix = self.px.predict(src)

        # 行列Eを用意する
        E = np.zeros((T, D * 2))
        for t in range(T):
            m = optimum_mix[t]
            # フレームごとにEtを代入する
            xx = np.linalg.solve(self.covarXX[m], src[t] - self.src_means[m])
            E[t] = self.tgt_means[m] + np.dot(self.covarYX[m], xx)
        E = E.flatten()

        # コンストラクタで計算したself.Dを元にD^-1の対角要素を計算する
        D_inv = np.zeros((T, D * 2, D * 2))
        for t in range(T):
            m = optimum_mix[t]
            # フレームごとに分布系列に対応するself.Dの逆行列を代入する
            D_inv[t] = np.linalg.inv(self.D[m])
        # 計算した要素を対角成分とする
        D_inv = scipy.sparse.block_diag(D_inv, format = 'csr')

        # 計算した行列を用いて変換後の特徴量を求める
        mutual = W.T.dot(D_inv)
        covar = mutual.dot(W)
        mean = mutual.dot(E)
        # numpy.linalg.solveに対応する疎行列向けのメソッドを使う
        y = scipy.sparse.linalg.spsolve(covar, mean, use_umfpack = False)

        return y.reshape((T, D))
</pre></list>
<p>以上で変換処理が実装できる。念のため、ほぼ情報量はないが、TrajectoryGMMMapの実装全体を載せておく。</p>
<codelist>
<caption>リスト5.2　trajectory.py</caption>
<pre><span type='lineno'> 1: </span>#!/usr/bin/python
<span type='lineno'> 2: </span># coding: utf-8
<span type='lineno'> 3: </span>
<span type='lineno'> 4: </span>import numpy as np
<span type='lineno'> 5: </span>
<span type='lineno'> 6: </span>from sklearn.mixture import GMM
<span type='lineno'> 7: </span>
<span type='lineno'> 8: </span>import scipy.sparse
<span type='lineno'> 9: </span>import scipy.sparse.linalg
<span type='lineno'>10: </span>
<span type='lineno'>11: </span>from gmmmap import GMMMap
<span type='lineno'>12: </span>
<span type='lineno'>13: </span>class TrajectoryGMMMap(GMMMap):
<span type='lineno'>14: </span>    def __init__(self, gmm, swap = False):
<span type='lineno'>15: </span>        &lt;省略&gt;
<span type='lineno'>16: </span>
<span type='lineno'>17: </span>    def __construct_weight_matrix(self, T, D):
<span type='lineno'>18: </span>        &lt;省略&gt;
<span type='lineno'>19: </span>
<span type='lineno'>20: </span>    def convert(self, src):
<span type='lineno'>21: </span>        &lt;省略&gt;
</pre></codelist>
<title aid:pstyle="h3">STFファイルへの変換</title><?dtp level="3" section="STFファイルへの変換"?>
<p>変換処理を呼び出し、結果をSTFファイルに保存するスクリプトは前章とほとんど変わらない。MFCCの動的変化量を結合するのと、フレームごとに呼び出していた変換処理をまとめて呼び出すように変更するのみである。</p>
<p>以下に、前章のスクリプトとトラジェクトリベースな変換のためのスクリプトのdiffを載せておく。</p>
<codelist>
<caption>リスト5.3　convert_gmmmap.pyとconvert_trajectory.pyのdiff</caption>
<pre><span type='lineno'> 1: </span>9c9
<span type='lineno'> 2: </span>&lt; from gmmmap import GMMMap
<span type='lineno'> 3: </span>---
<span type='lineno'> 4: </span>&gt; from trajectory import TrajectoryGMMMap
<span type='lineno'> 5: </span>38c38
<span type='lineno'> 6: </span>&lt;     source_data = numpy.array([mfcc.mfcc(source.SPEC[frame]) \
<span type='lineno'> 7: </span>---
<span type='lineno'> 8: </span>&gt;     source_mfcc = numpy.array([mfcc.mfcc(source.SPEC[frame]) \
<span type='lineno'> 9: </span>39a40
<span type='lineno'>10: </span>&gt;     source_data = numpy.hstack([source_mfcc, mfcc.delta(source_mfcc)])
<span type='lineno'>11: </span>41,42c42
<span type='lineno'>12: </span>&lt;     output_mfcc = numpy.array([gmmmap.convert(source_data[frame])[0] \
<span type='lineno'>13: </span>&lt;                                         for frame in xrange(source_data.shape[0])])
<span type='lineno'>14: </span>---
<span type='lineno'>15: </span>&gt;     output_mfcc = gmmmap.convert(source_data)
</pre></codelist>
<title aid:pstyle="h2">5.3　トラジェクトリベースな変換処理の結果</title><?dtp level="2" section="5.3　トラジェクトリベースな変換処理の結果"?>
<p>以下の画像は、トラジェクトリベースな変換の結果を表したグラフである。前章と同様に、それぞれのグラフはMFCCの第1次係数(数値が小さい方)及び第2次係数(数値が大きい方)の推移を表しており、上から、変換先話者のデータ、トラジェクトリベースな変換を行ったデータ、GMMによるフレームごとに独立な変換処理を行ったデータをDTWによって伸縮させたものである。中段のグラフの方が、下段よりなめらかに推移し、より上段の変換先話者のデータに近づいていることが分かるだろう。</p>
<img>
<Image href="file://images/trajectory-result.png" scale="0.6" />
<caption>図5.2　トラジェクトリベースな変換とフレームごとに独立な変換による結果データの比較</caption>
</img>
<p>このように、前後フレームとのつながりを考えた変換手法を採用することで、より自然な変換音声を生成することができる。</p>
</doc>
