<?xml version="1.0" encoding="UTF-8"?>
<doc xmlns:aid="http://ns.adobe.com/AdobeInDesign/4.0/"><title aid:pstyle="h1">第2章　統計的声質変換の基礎知識</title><?dtp level="1" section="第2章　統計的声質変換の基礎知識"?>
<title aid:pstyle="h2">2.1　統計的声質変換とは</title><?dtp level="2" section="2.1　統計的声質変換とは"?>
<p>統計的声質変換とは、人間の声から抽出された特徴量を統計的手法によって変換することによって、ある人の音声データを元に、別の人が同じ内容を話したかのような音声データを生成する仕組みのことである。基本的には、変換元となる人と変換先となる人が同じ内容を話しているデータ(以降、「パラレルデータ」という)を用いて、統計的モデルを学習することによって、変換を行っている。</p>
<p>ただし、この統計的モデルが扱えるのはあくまで数値データであり、音声データをそのまま学習に用いることはできない<footnote>厳密に言えば、音声データをそのまま数値化して扱うことはできるものの、データ量が大きすぎて学習に時間が掛かり過ぎる。</footnote>。</p>
<p>そこで、音声データを少ないパラメータで表すために、音声分析による特徴量抽出が必要になる。もちろん、統計的モデルによって変換されたパラメータから、音声合成によって音声データを復元する作業も必要になる。</p>
<p>つまり、統計的声質変換は、以下の様な仕組みとなっている。</p>
<img>
<Image href="file://images/frame.png" scale="0.3" />
<caption>図2.1　統計的声質変換の仕組み</caption>
</img>
<title aid:pstyle="h2">2.2　音声分析による特徴量抽出</title><?dtp level="2" section="2.2　音声分析による特徴量抽出"?>
<p>音声分析のベースには、ソースフィルタモデルと呼ばれる概念が取り入れられている。ソースフィルタモデルにおいては、声帯の振動によって生み出された音を声道や口腔の形状によって変化させることで発声しているという考え方に基づき、声帯からの音源(ソース)を表すパラメータと、声道や口腔での変化(フィルタ)を表すパラメータを用いて音声の特徴を表す。この際、ソースのパラメータは声の高さや太さ、フィルタのパラメータは「あ」「い」といった発声内容や声の個人性(人それぞれの特徴)などを表すこととなる。つまり、このフィルタを表すパラメータを統計的モデルによって変換することによって声質変換ができる。</p>
<img>
<Image href="file://images/source-filter.png" scale="0.3" />
<caption>図2.2　ソースフィルタモデル</caption>
</img>
<p>今回は、このパラメータの抽出については、TANDEM-STRAIGHT<span type='bibref' idref='tandem-straight'>[1]</span>というシステムを利用する。これは、山梨大学の森勢助教が公開している<footnote>http://ml.cs.yamanashi.ac.jp/straight/</footnote>音声分析・合成システムであり、音声データを分析し、ソースを表すパラメータである基本周波数、フィルタを表すパラメータであるスペクトル包絡、そして、音声のかすれや雑音を表すパラメータである非周期成分という3つのパラメータを抽出することができる。もちろん、この3つのパラメータから音声データを合成することも可能である。</p>
<p>さらに、メル周波数ケプストラム係数(以降、「MFCC」という)というものを導入する。これは、スペクトル包絡をより人間の知覚に沿うような形で抽出した特徴量であり、具体的には、低い音に対してはその音程の細かな違いに気づくが、高い音になるほど音程の違いに気づきにくくなるという人間の特性を利用している。MFCCは、スペクトル包絡に対して低い周波数帯がより強調されるようなフィルタ(メルバンクフィルタという)を適用し、離散コサイン変換を行うことによって得られる。</p>
<title aid:pstyle="h2">2.3　EMアルゴリズム</title><?dtp level="2" section="2.3　EMアルゴリズム"?>
<p>統計的声質変換には、後述する混合ガウスモデルがよく使われるのだが、それだけに限らず様々な場面で使われるのがEMアルゴリズムである。EMアルゴリズムは、確率モデルのパラメータを推定するときに使われる手法で、ある観測データ<replace idref="texinline-1"><pre>\boldsymbol{X}</pre></replace>、その観測データと潜在変数<replace idref="texinline-2"><pre>\boldsymbol{Z}</pre></replace>との同時分布<replace idref="texinline-3"><pre>P(\boldsymbol{X}, \boldsymbol{Y} | \boldsymbol{\theta})</pre></replace>が与えられた時に、<replace idref="texinline-4"><pre>P(\boldsymbol{X} | \boldsymbol{\theta}</pre></replace>}を最大化するようなパラメータ<replace idref="texinline-5"><pre>\boldsymbol{\theta}</pre></replace>を推定することができる。式で表すと、以下の通りとなる。</p>
<replace idref="texblock-1">
<pre>
    \begin{split}
        \hat{\boldsymbol{\theta}} = &amp; \ \underset{\boldsymbol{\theta}}{argmax} \ P(\boldsymbol{X} | \boldsymbol{\theta}) \\
                                  = &amp; \ \underset{\boldsymbol{\theta}}{argmax} \ \prod_{all \ \boldsymbol{Z}} P(\boldsymbol{X}, \boldsymbol{Z} | \boldsymbol{\theta})
    \end{split}
</pre>
</replace>
<p>ここで、Q関数と呼ばれる関数を導入する。</p>
<replace idref="texblock-2">
<pre>
    \mathcal{Q} (\boldsymbol{\theta}, \boldsymbol{\theta} ^ {old}) = \sum_{all \ \boldsymbol{Z}} P(\boldsymbol{Z} | \boldsymbol{X}, \boldsymbol{\theta} ^ {old}) \log P(\boldsymbol{X}, \boldsymbol{Z} | \boldsymbol{\theta})
</pre>
</replace>
<p>すると、以下のステップによって、<replace idref="texinline-6"><pre>\boldsymbol{\theta}</pre></replace>を推定できる。</p>
<ol>
<li aid:pstyle="ol-item" olnum="1" num="1">パラメータの初期値<replace idref="texinline-7"><pre>\boldsymbol{\theta} ^ {old}</pre></replace>を適当に定める</li>
<li aid:pstyle="ol-item" olnum="2" num="2">Eステップ: <replace idref="texinline-8"><pre>P(\boldsymbol{Z} | \boldsymbol{X}, \boldsymbol{\theta} ^ {old})</pre></replace>を計算する。</li>
<li aid:pstyle="ol-item" olnum="3" num="3">Mステップ: Eステップで得られた値を元に<replace idref="texinline-9"><pre>\boldsymbol{\theta} ^ {new} = \underset{\boldsymbol{\theta}}{argmax} \mathcal{Q} (\boldsymbol{\theta}, \boldsymbol{\theta} ^ {new})</pre></replace>を計算する。</li>
<li aid:pstyle="ol-item" olnum="4" num="4"><replace idref="texinline-10"><pre>\boldsymbol{\theta}</pre></replace>が収束するまで<replace idref="texinline-11"><pre>\boldsymbol{\theta} ^ {old} \leftarrow \boldsymbol{\theta} ^ {new}</pre></replace>としてEステップとMステップを繰り返す。</li>
</ol>
<p>なぜこの方法によって<replace idref="texinline-12"><pre>\boldsymbol{\theta}</pre></replace>の最尤推定ができるのかという詳しい説明は専門書に譲るとして、図を用いて簡単に説明をしておく。以下の図は、<replace idref="texinline-13"><pre>\boldsymbol{\theta}</pre></replace>と対数尤度関数<replace idref="texinline-14"><pre>\log P(\boldsymbol{X} | \boldsymbol{\theta})</pre></replace>のグラフである。</p>
<img>
<Image href="file://images/em.png" scale="0.28" />
<caption>図2.3　EMアルゴリズムのイメージ</caption>
</img>
<p>実は、Eステップは対数尤度関数の<replace idref="texinline-15"><pre>\boldsymbol{\theta} ^ {old}</pre></replace>における下界を求めるというのと対応し、Mステップはその下界を最大化するパラメータ<replace idref="texinline-16"><pre>\boldsymbol{\theta} ^ {new}</pre></replace>を求めるというのと対応している。図を見れば、それを繰り返すことによって対数尤度関数を最大化するようにパラメータが動いていることが分かるだろう。</p>
<title aid:pstyle="h2">2.4　混合ガウスモデル</title><?dtp level="2" section="2.4　混合ガウスモデル"?>
<p>混合ガウスモデル(以降、「GMM」という)は、複数のガウス分布を組み合わせることによって表されるモデルである。扱うデータを<replace idref="texinline-17"><pre>D</pre></replace>次元のベクトル<replace idref="texinline-18"><pre>\boldsymbol{x}</pre></replace>としたときの定義は以下の通りである。</p>
<replace idref="texblock-3">
<pre>
    P(\boldsymbol{x} | \boldsymbol{\lambda}) = \sum_{m = 1}^{M} w_m \mathcal{N} (\boldsymbol{x}; \boldsymbol{\mu}_m, \boldsymbol{\Sigma}_m)
</pre>
</replace>
<p>ここで、<replace idref="texinline-19"><pre>\mathcal{N} (\boldsymbol{x}; \boldsymbol{\mu}_m, \boldsymbol{\Sigma}_m)</pre></replace>は、平均ベクトルが<replace idref="texinline-20"><pre>\boldsymbol{\mu}_m</pre></replace>、分散共分散行列が<replace idref="texinline-21"><pre>\boldsymbol{\Sigma}_m</pre></replace>のガウス分布である。</p>
<replace idref="texblock-4">
<pre>
    \mathcal{N} (\boldsymbol{x}; \boldsymbol{\mu}_m, \boldsymbol{\Sigma}_m) = \frac{1}{(2 \pi) ^ \frac{d}{2} \boldsymbol{\Sigma}_m} \exp(- \frac{1}{2} (\boldsymbol{x} - \boldsymbol{\mu}_m) ^ \top \boldsymbol{\Sigma}_m ^ {-1} (\boldsymbol{x} - \boldsymbol{\mu}_m))
</pre>
</replace>
<p>すなわち、GMMは<replace idref="texinline-22"><pre>M</pre></replace>個のガウス分布を線形結合したものであり、確率密度関数となるために以下の制約を持つ。</p>
<replace idref="texblock-5">
<pre>
    w_1, ..., w_m \geq 0, \sum_{m = 1}^{M} w_m = 1
</pre>
</replace>
<p>つまり、GMMのパラメータは、それぞれのガウスモデルの重みと平均ベクトル、分散共分散行列からなり、まとめて<replace idref="texinline-23"><pre>\boldsymbol{\lambda}</pre></replace>として表される。</p>
<title aid:pstyle="h3">混合ガウスモデルの最尤推定</title><?dtp level="3" section="混合ガウスモデルの最尤推定"?>
<p>GMMの学習は、学習データのそれぞれに対して推定される確率の積が最大となるようなパラメータを見つけることである。つまり、<replace idref="texinline-24"><pre>N</pre></replace>個の学習データに対し、尤度関数<replace idref="texinline-25"><pre>L(\boldsymbol{\lambda})</pre></replace>を用いて以下のように定式化される。</p>
<replace idref="texblock-6">
<pre>
    L(\boldsymbol{\lambda}) := \prod_{n = 1}^{N} P(\boldsymbol{x}_n, \boldsymbol{\lambda}) \\\\
</pre>
</replace>
<replace idref="texblock-7">
<pre>
    \hat{\boldsymbol{\lambda}} := \underset{\boldsymbol{\lambda}}{argmax} \ L(\boldsymbol{\lambda}) \ \text{subject to}
    \left\{
        \begin{array}{l}
            w_1, ..., w_m \geq 0 \\
            \sum_{m = 1}^{M} w_m = 1
        \end{array}
    \right.
</pre>
</replace>
<p>このとき、最尤推定量<replace idref="texinline-26"><pre>\hat{\boldsymbol{\lambda}}</pre></replace>は次式を満たす。</p>
<replace idref="texblock-8">
<pre>
    \left. \frac{\partial}{\partial \boldsymbol{\lambda}} L(\boldsymbol{\lambda}) \right|_{\boldsymbol{\lambda} = \hat{\boldsymbol{\lambda}}} = 0
</pre>
</replace>
<p>これは、重み、平均ベクトル、分散共分散行列について、それぞれ以下を満たす。</p>
<replace idref="texblock-9">
<pre>
    \hat{w}_m = \frac{1}{N} \sum_{n = 1}^{N} \hat{\eta}_{n, m}
</pre>
</replace>
<replace idref="texblock-10">
<pre>
    \hat{\boldsymbol{\mu}}_m = \frac{ \sum_{n = 1}^{N} \hat{\eta}_{n, m} \boldsymbol{x}_n }{ \sum_{n = 1}^{N} \hat{\eta}_{n, m} }
</pre>
</replace>
<replace idref="texblock-11">
<pre>
    \hat{\boldsymbol{\Sigma}}_m = \frac{ \sum_{n = 1}^{N} \hat{\eta}_{n, m} (\boldsymbol{x}_n - \hat{\boldsymbol{\mu}}_m) ^ \top (\boldsymbol{x}_n - \hat{\boldsymbol{\mu}}_m) }{ d \sum_{n = 1}^{N} \hat{\eta}_{n, m} }
</pre>
</replace>
<p>ただし、<replace idref="texinline-27"><pre>\hat{\eta}_{n, m}</pre></replace>は次式の通りである。</p>
<replace idref="texblock-12">
<pre>
    \hat{\eta}_{n, m} := \frac{ \hat{w}_m \mathcal{N} (x_n; \hat{\boldsymbol{\mu}}_m, \hat{\boldsymbol{\Sigma}}_m) }{ \sum_{m' = 1}^{M} \hat{w}_m \mathcal{N} (x_n; \hat{\boldsymbol{\mu}}_{m'}, \hat{\boldsymbol{\Sigma}}_{m'}) }
</pre>
</replace>
<p>これは、<replace idref="texinline-28"><pre>\hat{\eta}_{n, m}</pre></replace>の定義にそれぞれのパラメータが用いられているため、パラメータを解析的に求めることは難しいので、EMアルゴリズムを用いることで推定することができる。この場合は、各ステップは以下のようになる。</p>
<ol>
<li aid:pstyle="ol-item" olnum="1" num="1">パラメータの初期値<replace idref="texinline-29"><pre>\hat{w}, \hat{\boldsymbol{\mu}}, \hat{\boldsymbol{\Sigma}}</pre></replace>を適当に定める。</li>
<li aid:pstyle="ol-item" olnum="2" num="2">Eステップ: 現在のパラメータから<replace idref="texinline-30"><pre>\hat{\eta}_{n, m}</pre></replace>を計算する。</li>
<li aid:pstyle="ol-item" olnum="3" num="3">Mステップ: 現在の<replace idref="texinline-31"><pre>\hat{\eta}_{n, m}</pre></replace>から<replace idref="texinline-32"><pre>\hat{w}, \hat{\boldsymbol{\mu}}, \hat{\boldsymbol{\Sigma}}</pre></replace>を計算する。</li>
<li aid:pstyle="ol-item" olnum="4" num="4">収束するまでEステップとMステップを繰り返す。</li>
</ol>
<p>これにより、学習データに対する尤度を最大化するようなパラメータを推定することができる。その他、EMアルゴリズムの導出やGMMについての具体的な仕組みなどは、C.M. ビショップ著「パターン認識と機械学習」<span type='bibref' idref='prml'>[2]</span>や杉山 将著「統計的機械学習」<span type='bibref' idref='stat-ml'>[3]</span>などを参照されたい。</p>
</doc>
